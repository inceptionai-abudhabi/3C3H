# 3C3H Evaluation Pipeline

A fullyâ€“automated, **Apacheâ€‘2.0â€“licensed** toolkit that generates answers from language models (openâ€‘weight *and* proprietary), judges them with LLMâ€‘asâ€‘aâ€‘Judge following the **3C3H** (Correctness, Completeness, Conciseness, Helpfulness, Honesty, Harmlessness) evaluation score criterias which were first introduced [here](https://huggingface.co/blog/leaderboard-3c3h-aragen) and produces leaderboardâ€‘ready scores to report.

> **Why?**
> Measuring the realâ€‘world utility and safety of LLMs at scale requires a reproducible, endâ€‘toâ€‘end pipeline. This repository powers currently the [*Arabic Leaderboards*](https://huggingface.co/spaces/inceptionai/Arabic-Leaderboards) and related internal evaluations at **G42**'sÂ **Inception**.

---

## âœ¨Â Key Features

| Phase | Script | Highlights |
|-------|--------|------------|
| **GenerationÂ â€” Open Models** | `pipeline/generate-oma.py` | Local HF models (Text & TextÂ +Â Vision), automatic multiâ€‘GPU sharding, chatâ€template detection. |
| **GenerationÂ â€” Proprietary Models** | `pipeline/generate-pma.py` | Unified wrapper over OpenAI, Anthropic, GoogleÂ Gemini, DeepSeek, Mistral, xAIÂ Grok, and Inception APIs with key autoâ€‘discovery and rateâ€‘limit handling. |
| **Judging** | `pipeline/jury.py` | Multiâ€‘judge voting or averaging, ClaudeÂ 3.5 & GPTâ€‘4o support, perâ€‘round prompts, zeroâ€‘score propagation, JSON extraction robustness. |
| **Aggregation** | `pipeline/averaging-to-results.py` | Merges *_judged.json* files, computes perâ€‘judge and perâ€‘task averages, writes batchâ€‘scoped `results/*.json`. |
| **Orchestration** | `pipeline/run-pipeline.sh` | SLURM job: model sync â†’ generation â†’ judging â†’ aggregation â†’ (optional) HFÂ Hub sync. |

---

## ğŸ—‚ï¸ Repository Layout

```
â”œâ”€â”€ tasks/                     # Input datasets (e.g. AraGen-12â€‘2024.json)
â”œâ”€â”€ models/                    # Local cache of openâ€‘weight models (Optional)
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ generate-oma.py        # Openâ€‘weight answer generation
â”‚   â”œâ”€â”€ generate-pma.py        # Proprietary answer generation
â”‚   â”œâ”€â”€ jury.py                # LLMâ€‘asâ€‘aâ€‘Judge
â”‚   â”œâ”€â”€ averaging-to-results.py
â”‚   â”œâ”€â”€ run-pipeline.sh        # SLURM pipeline script
â”‚   â”œâ”€â”€ requirements.txt       # requirements file to be installed
â”‚   â”œâ”€â”€ utils/                 # Sync helpers (Hub â†” local)
â”‚   â””â”€â”€ prompts/               # Different versions of the Judge System Prompts
â”œâ”€â”€ requests/                  # Task-specific model requests
â”‚   â””â”€â”€ <task-name>/           # e.g. AraGen-12-2024-dev/
â”‚       â”œâ”€â”€ OpenRequests/      # Open-source model requests
â”‚       â””â”€â”€ ProprietaryRequests/ # Proprietary model requests
â”œâ”€â”€ answers/                   # Generated model answers
â”‚   â””â”€â”€ <task-name>/          # Task-specific answers
â”œâ”€â”€ results/                   # Aggregated 3C3H metrics
â”‚   â””â”€â”€ <task-name>/          # Task-specific results
â””â”€â”€ logs/                     # Pipeline execution logs
    â””â”€â”€ <task-name>/          # Task-specific logs
        â””â”€â”€ <job-id>/         # Job-specific logs
```

---

## âš¡ Quick Start

```bash
# 1. Clone
$ git clone https://github.com/inceptionai-abudhabi/3C3H.git
$ cd 3C3H

# 2. Create environment (Conda example)
$ conda create -n 3c3h python=3.10 -y
$ conda activate 3c3h
$ pip install -r ./pipeline/requirements.txt

# 3. Export **all** required API keys & HF token
$ export HF_TOKEN="<your-hf-token>"
$ export ANTHROPIC_API_KEY="..."
# ... OPENAI_API_KEY, GOOGLE_API_KEY, DEEPSEEK_API_KEY, MISTRAL_API_KEY, XAI_API_KEY

# 4. (Optional) Download models aheadâ€‘ofâ€‘time
$ huggingface-cli download "inceptionai/jais-family-13b-chat" --local-dir models/inceptionai/jais-family-13b-chat

# 5. Launch the pipeline with desired options (Make sure to edit the SLURM header first):

# Process all pending requests with defaults:
$ sbatch ./pipeline/run-pipeline.sh

# Process specific model:
$ sbatch ./pipeline/run-pipeline.sh --model inceptionai/jais-family-13b-chat

# Process multiple models (mix of open and proprietary):
$ sbatch ./pipeline/run-pipeline.sh --model "inceptionai/jais-family-13b-chat,gpt-4o,claude-3-sonnet"

# Process with custom parameters:
$ sbatch ./pipeline/run-pipeline.sh --model inceptionai/jais-family-13b-chat \
  --task AraGen-12-2024 \
  --precision float16 \
  --params 13B \
  --env custom-env
```

## âš™ï¸ Configuration Reference

### Command Line Arguments

| Argument | Description | Default |
|----------|-------------|---------|
| `--task` | Task name | AraGen-12-2024-dev |
| `--model` | Model name(s) - comma-separated for multiple | (all pending) |
| `--env` | Conda environment name | 3c3h |
| `--license` | License type | Open |
| `--revision` | Model revision | main |
| `--precision` | Model precision | bfloat16 |
| `--params` | Model parameters | 0 |
| `--status` | Model status | RUNNING |
| `--modality` | Model modality | Text (Accepted values: "Text" || "Text+Vision")|

### Environment Variables

| Variable | Required | Purpose |
|----------|----------|---------|
| `HF_TOKEN` | **Yes** | Authenticate to HuggingÂ Face for dataset & model pulls/pushes. |
| `OPENAI_API_KEY` | Yes (if using GPTâ€‘4/4o) | OpenAI models. |
| `ANTHROPIC_API_KEY` | Yes (if using Claude) | Anthropic models. |
| `GOOGLE_API_KEY` | Yes (if using Gemini) | Google GenerativeÂ AI. |
| `DEEPSEEK_API_KEY` | Yes (if using DeepSeek) | DeepSeek models. |
| `MISTRAL_API_KEY` | Yes (if using MistralÂ AI) | Mistral models. |
| `XAI_API_KEY` | Yes (if using Grok) | xAI models. |
| `INCEPTION_API_KEY` | Internal Only | JAIS,Â K2, etc. |
| `INCEPTION_LLAMA3P1_405B_API_KEY` | Internal Only | LlamaÂ 3.1â€‘405B endpoint. |

### SLURM Resources
Modify the header in `run-pipeline.slurm` to match your cluster:

```bash
#SBATCH --job-name=3c3h
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=8
#SBATCH --cpus-per-task=12
#SBATCH --gpus-per-node=8
#SBATCH --mem=800G
#SBATCH --time=14-00:00:00
#SBATCH --partition=your-partition-name
```

### Dataset Batches
Place your evaluation JSON in `tasks/` and specify the task name using the `--task` argument (defaults to AraGen-12-2024-dev).

---

## ğŸ“ˆÂ Outputs

| Path | Description |
|------|-------------|
| `answers/<task_name>/<org>_<model>_<rev>_<prec>_answers.json` | Raw model completions. |
| `answers/<task_name>/*_answers_judged.json` | Same file after *jury.py* stores judge scores & comments. |
| `results/<task_name>/results__strategy_<vote_or_average>.json` | Aggregated 3C3H & perâ€‘task scores â€” ready for a leaderboard display. |
| `logs/<task_name>/<SLURM_JOB_ID>/` | Generation & judging stdout/stderr + success/failure model lists. |

---

## ğŸ–‡ï¸Â Contributing

Pull requests welcome! Please ensure:

- Code is **PEPÂ 8** compliant and typed where practical.
- New dependencies are added to `requirements.txt`.
- Unit tests passed if applicable.

---

## ğŸ“œÂ License

```
Copyright 2025 G42 General Trading LLC

Licensed under the Apache License, VersionÂ 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```
